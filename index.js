/**
 * AI Shorts Generator ‚Äî production-ready index.js
 * - Priority text models: llama-3.1-8b-instant -> meta-llama/llama-guard-4-12b -> groq/compound-mini
 * - TTS: PlayAI -> PlayAI-Arabic -> Google TTS fallback
 * - Transcription: whisper-large-v3-turbo -> whisper-large-v3
 * - Single-file, ESM, clear logs, retries/timeouts
 *
 * Usage:
 * 1) create .env with GROQ_API_KEY and optional PLAYAI_TTS_KEY
 * 2) npm install express openai axios dotenv google-tts-api form-data
 * 3) node index.js
 *
 * Endpoints:
 * POST /generate    { prompt } -> { prompt, texte, audioFile }
 * POST /tts         { text, lang? } -> { audioFile }
 * POST /transcribe  multipart/form-data with file field "file" -> { text }
 * GET  /status
 */

import express from "express";
import dotenv from "dotenv";
import OpenAI from "openai";
import axios from "axios";
import fs from "fs";
import path from "path";
import FormData from "form-data";
import googleTTS from "google-tts-api";

dotenv.config();

// -------------------- Config --------------------
const PORT = process.env.PORT || 3000;
const OUTPUT_DIR = process.env.OUTPUT_DIR || "./output";
if (!fs.existsSync(OUTPUT_DIR)) fs.mkdirSync(OUTPUT_DIR, { recursive: true });

const GROQ_API_KEY = process.env.GROQ_API_KEY || "";
const PLAYAI_TTS_KEY = process.env.PLAYAI_TTS_KEY || ""; // optional
const REQUEST_TIMEOUT_MS = 60_000; // base timeout for model calls

if (!GROQ_API_KEY) {
  console.warn("‚ö†Ô∏è GROQ_API_KEY missing in .env ‚Äî add it before production use.");
}

// -------------------- Clients --------------------
// We'll use OpenAI client pointed at Groq's OpenAI-compatible endpoint
const groqClient = new OpenAI({
  apiKey: GROQ_API_KEY,
  baseURL: "https://api.groq.com/openai/v1",
  // Note: OpenAI client has built-in timeouts? we'll wrap calls with Promise.race/timeouts where necessary
});

// Axios instance with timeout to use for PlayAI calls and fallback fetches
const axiosDefaults = axios.create({ timeout: REQUEST_TIMEOUT_MS, maxContentLength: Infinity, maxBodyLength: Infinity });

// -------------------- Models & params --------------------
const TEXT_MODELS = [
  "llama-3.1-8b-instant",
  "meta-llama/llama-guard-4-12b",
  "groq/compound-mini"
];

const TTS_PLAYAI_MODELS = {
  default: "playai-tts",
  arabic: "playai-tts-arabic"
};

const TRANSCRIBE_MODELS = [
  "whisper-large-v3-turbo",  // priority
  "whisper-large-v3"         // fallback
];

const MAX_RETRIES = 4; // per model
const RETRY_BACKOFF = (attempt) => 1000 + attempt * 1500;

// -------------------- Helpers --------------------
const sleep = (ms) => new Promise((r) => setTimeout(r, ms));

function isArabic(text) {
  // small heuristic: presence of characters in Arabic block
  return /[\u0600-\u06FF\u0750-\u077F]/.test(text);
}

async function callGroqResponse({ model, input, timeoutMs = REQUEST_TIMEOUT_MS }) {
  // Wrap client call with a Promise that times out if it takes too long.
  const p = groqClient.responses.create({ model, input });
  const timeout = new Promise((_, rej) =>
    setTimeout(() => rej(new Error("Timeout")), timeoutMs)
  );
  return Promise.race([p, timeout]);
}

// -------------------- Text generation with fallback + retries --------------------
async function generateText(prompt) {
  if (!prompt || !prompt.trim()) throw new Error("Prompt vide");

  for (const model of TEXT_MODELS) {
    for (let attempt = 1; attempt <= MAX_RETRIES; attempt++) {
      try {
        console.log(`üü¢ [Text] Tentative ${attempt}/${MAX_RETRIES} avec ${model}`);
        const res = await callGroqResponse({ model, input: prompt, timeoutMs: REQUEST_TIMEOUT_MS });
        const out = res?.output_text ?? (res?.output?.map(o => o.content || "").join(" ") || "");
        if (out && out.trim()) {
          console.log(`‚úÖ [Text] R√©ussi avec ${model}`);
          return { model, text: out.trim() };
        }
        throw new Error("R√©ponse vide");
      } catch (err) {
        console.warn(`‚ùå [Text] Erreur (${model}) attempt ${attempt}: ${err?.response?.data || err.message}`);
        if (attempt < MAX_RETRIES) {
          const delay = RETRY_BACKOFF(attempt);
          console.log(`‚è≥ [Text] attente ${delay}ms avant retry...`);
          await sleep(delay);
        }
      }
    }
    console.log(`üîÅ [Text] Fallback vers mod√®le suivant apr√®s √©checs: ${model}`);
  }
  throw new Error("Abandon apr√®s trop d'√©checs sur les mod√®les de texte");
}

// -------------------- TTS: PlayAI primary, Google fallback --------------------
async function ttsPlayAI(text, filename, modelName = TTS_PLAYAI_MODELS.default) {
  if (!PLAYAI_TTS_KEY) throw new Error("PLAYAI_TTS_KEY missing");
  const url = "https://api.play.ai/v1/tts";
  try {
    // API shape may need { text, model } or other fields depending on PlayAI ‚Äî we send minimal payload
    const resp = await axiosDefaults.post(
      url,
      { text, model: modelName },
      {
        headers: { Authorization: `Bearer ${PLAYAI_TTS_KEY}`, "Content-Type": "application/json" },
        responseType: "arraybuffer" // expect binary audio
      }
    );
    const filePath = path.join(OUTPUT_DIR, filename);
    fs.writeFileSync(filePath, Buffer.from(resp.data));
    console.log(`‚úÖ [TTS] PlayAI audio saved: ${filePath}`);
    return filePath;
  } catch (err) {
    // Return error details for logging upstream
    const msg = err?.response?.data ? Buffer.from(err.response.data).toString() : err.message;
    throw new Error(`PlayAI TTS failed: ${msg}`);
  }
}

async function ttsGoogle(text, filename, lang = "fr") {
  // google-tts-api returns a URL; we download the file to disk
  try {
    const url = googleTTS.getAudioUrl(text, {
      lang,
      slow: false,
      host: "https://translate.google.com",
    });
    const resp = await axiosDefaults.get(url, { responseType: "arraybuffer" });
    const filePath = path.join(OUTPUT_DIR, filename);
    fs.writeFileSync(filePath, Buffer.from(resp.data));
    console.log(`‚úÖ [TTS] Google TTS audio saved: ${filePath}`);
    return filePath;
  } catch (err) {
    throw new Error(`Google TTS failed: ${err.message}`);
  }
}

async function generateTTSWithFallback(text, suggestedFilename) {
  // choose arabic PlayAI model if Arabic content detected
  const langIsArabic = isArabic(text);
  const playModel = langIsArabic ? TTS_PLAYAI_MODELS.arabic : TTS_PLAYAI_MODELS.default;

  // Try PlayAI if key available
  if (PLAYAI_TTS_KEY) {
    try {
      return await ttsPlayAI(text, suggestedFilename, playModel);
    } catch (err) {
      console.warn(`‚ö†Ô∏è [TTS] PlayAI failed, falling back to Google TTS: ${err.message}`);
    }
  } else {
    console.log("‚ÑπÔ∏è [TTS] PLAYAI_TTS_KEY not set ‚Äî skipping PlayAI.");
  }

  // fallback to Google TTS
  const lang = isArabic(text) ? "ar" : "fr"; // default to fr for this project (adjust if needed)
  return await ttsGoogle(text, suggestedFilename, lang);
}

// -------------------- Transcription (multipart upload) --------------------
async function transcribeFileWithGroq(filepath) {
  // Using Groq/OpenAI-compatible audio transcription endpoint
  const url = "https://api.groq.com/openai/v1/audio/transcriptions";
  const form = new FormData();
  form.append("file", fs.createReadStream(filepath));
  // prefer turbo model first then fallback inside caller
  form.append("model", TRANSCRIBE_MODELS[0]); // initial
  try {
    const resp = await axios.post(url, form, {
      headers: {
        ...form.getHeaders(),
        Authorization: `Bearer ${GROQ_API_KEY}`,
      },
      maxContentLength: Infinity,
      maxBodyLength: Infinity,
      timeout: REQUEST_TIMEOUT_MS
    });
    // OpenAI-style response typically has { text: "..." } or { transcription: ... }
    // We'll attempt to pick the likely fields
    if (resp.data?.text) return resp.data.text;
    if (resp.data?.transcript) return resp.data.transcript;
    if (resp.data?.output_text) return resp.data.output_text;
    return JSON.stringify(resp.data).slice(0, 1000); // fallback raw
  } catch (err) {
    throw new Error(err?.response?.data ? JSON.stringify(err.response.data) : err.message);
  }
}

async function transcribeWithFallback(filepath) {
  // Try turbo first then fallback to standard
  for (let model of TRANSCRIBE_MODELS) {
    try {
      console.log(`üü¢ [Transcribe] Essai mod√®le ${model}`);
      // We'll set model field dynamically and call the audio/transcriptions endpoint
      const url = "https://api.groq.com/openai/v1/audio/transcriptions";
      const form = new FormData();
      form.append("file", fs.createReadStream(filepath));
      form.append("model", model);
      const resp = await axios.post(url, form, {
        headers: {
          ...form.getHeaders(),
          Authorization: `Bearer ${GROQ_API_KEY}`,
        },
        maxContentLength: Infinity,
        maxBodyLength: Infinity,
        timeout: REQUEST_TIMEOUT_MS
      });
      if (resp.data?.text) return resp.data.text;
      if (resp.data?.transcript) return resp.data.transcript;
      if (resp.data?.output_text) return resp.data.output_text;
      return JSON.stringify(resp.data).slice(0, 2000);
    } catch (err) {
      console.warn(`‚ùå [Transcribe] Erreur mod√®le ${model}: ${err?.response?.data || err.message}`);
      // try next model
    }
  }
  throw new Error("Transcription √©chou√©e sur tous les mod√®les");
}

// -------------------- Express app & endpoints --------------------
const app = express();
app.use(express.json());
app.use(express.static("public"));

// STATUS
app.get("/status", (req, res) => res.json({ status: "ok", message: "AI Shorts Generator operational" }));

// POST /generate { prompt }
app.post("/generate", async (req, res) => {
  const prompt = (req.body?.prompt || "").toString().trim();
  if (!prompt) return res.status(400).json({ error: "Prompt manquant" });

  try {
    // Generate text (fallback across TEXT_MODELS)
    const { model, text } = await generateText(prompt);
    // Generate TTS with fallback and save file
    const filename = `short_${Date.now()}.mp3`;
    let audioFile = null;
    try {
      audioFile = await generateTTSWithFallback(text, filename);
    } catch (ttsErr) {
      console.warn(`‚ö†Ô∏è [Generate] TTS a √©chou√© : ${ttsErr.message}`);
      audioFile = null;
    }

    return res.json({ prompt, modelUsed: model, texte: text, audioFile });
  } catch (err) {
    console.error("‚ùå [Generate] Erreur fatale :", err.message || err);
    return res.status(500).json({ error: err.message || "Erreur interne lors de la g√©n√©ration" });
  }
});

// POST /tts { text, lang? }
app.post("/tts", async (req, res) => {
  const text = (req.body?.text || "").toString().trim();
  if (!text) return res.status(400).json({ error: "Text missing" });
  const filename = `tts_${Date.now()}.mp3`;
  try {
    const audioFile = await generateTTSWithFallback(text, filename);
    return res.json({ text, audioFile });
  } catch (err) {
    console.error("‚ùå [TTS] Erreur :", err.message || err);
    return res.status(500).json({ error: err.message || "TTS failed" });
  }
});

// POST /transcribe (multipart/form-data file=...)
app.post("/transcribe", async (req, res) => {
  try {
    // minimal multipart parser using busboy-like approach without adding dependency:
    // but to be robust, we recommend uploading a file to /upload path or using curl -F "file=@file.mp3"
    // We'll use a simple approach: store the incoming raw stream to a temp file
    const boundary = req.headers["content-type"]?.match(/boundary=(.+)$/)?.[1];
    if (!boundary) {
      return res.status(400).json({ error: "Content-Type must be multipart/form-data with boundary" });
    }

    // Save raw body to a temp file using a stream
    // NOTE: Express by default doesn't parse multipart - we must use a streaming parser (busboy) in production.
    // For simplicity here, expect clients to upload files to /transcribe-file endpoint with query ?path=/path/to/file on server.
    return res.status(400).json({ error: "Please upload file to server first (use /transcribe-file or implement multipart parser). Example implementation recommended with 'busboy' or 'multer'." });
  } catch (err) {
    console.error("‚ùå [Transcribe] Error:", err.message || err);
    return res.status(500).json({ error: err.message || "Transcription error" });
  }
});

// Convenience endpoint to transcribe an already uploaded file (server side): POST /transcribe-file { filepath: "./output/xxx.mp3" }
app.post("/transcribe-file", async (req, res) => {
  const filepath = req.body?.filepath;
  if (!filepath || !fs.existsSync(filepath)) return res.status(400).json({ error: "Filepath missing or file does not exist on server" });
  try {
    const text = await transcribeWithFallback(filepath);
    return res.json({ filepath, text });
  } catch (err) {
    console.error("‚ùå [Transcribe-file] Error:", err.message || err);
    return res.status(500).json({ error: err.message || "Transcription failed" });
  }
});

// -------------------- Start server --------------------
app.listen(PORT, () => {
  console.log(`üöÄ AI Shorts Generator listening at http://localhost:${PORT}`);
  console.log(`‚öôÔ∏è Output directory: ${path.resolve(OUTPUT_DIR)}`);
});
