// ==============================================
// AI Shorts Generator ‚Äì index.js
// Version finale top 0,1% ‚Äì GitHub ‚Üí Render Ready
// ==============================================

import express from 'express';
import cors from 'cors';
import dotenv from 'dotenv';
import { InferenceClient } from '@huggingface/inference';

dotenv.config(); // Charge HF_TOKEN et PORT depuis .env

const app = express();

// Middlewares
app.use(cors());
app.use(express.json());
app.use(express.static('public')); // Sert les fichiers HTML/CSS/JS c√¥t√© client

// Cr√©ation du client Hugging Face
const client = new InferenceClient({ token: process.env.HF_TOKEN });

// Endpoint principal pour g√©n√©rer du texte depuis l‚ÄôIA
app.post('/chat', async (req, res) => {
  const { message } = req.body;

  if (!message || message.trim() === '') {
    return res.status(400).json({ error: 'Le message est vide.' });
  }

  try {
    const response = await client.textGeneration({
      model: 'gpt2',           // Mod√®le Hugging Face, change si n√©cessaire
      inputs: message,
      parameters: { max_new_tokens: 100 },
    });

    res.json({ reply: response.generated_text });
  } catch (err) {
    console.error('Erreur Hugging Face :', err);
    res.status(500).json({ error: 'Erreur serveur. Veuillez r√©essayer.' });
  }
});

// Port dynamique pour Render / GitHub ‚Üí Render
const PORT = process.env.PORT || 3000;

// Gestion d‚Äôerreur pour port d√©j√† utilis√©
app.listen(PORT, () => {
  console.log(`üöÄ Serveur d√©marr√© sur le port ${PORT}`);
  console.log('üí° Endpoint /chat pr√™t √† recevoir des messages.');
}).on('error', (err) => {
  if (err.code === 'EADDRINUSE') {
    console.error(`‚ö†Ô∏è Le port ${PORT} est d√©j√† utilis√©.`);
  } else {
    console.error('Erreur serveur :', err);
  }
});
